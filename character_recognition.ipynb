{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "character_recognition.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOVFf/mJtCO6KG3FjghBku9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/patricksabry1/42028-Deep-Learning/blob/master/character_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssLx4iFxm4uU",
        "colab_type": "text"
      },
      "source": [
        "# Character Recognition model Using OpenCV & deep CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjBX2noqmFSW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9f20812b-5734-4700-a69c-c467ba1a52e7"
      },
      "source": [
        "import numpy as np\n",
        "import cv2 as cv\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxCZzObesycV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0f5b3956-f947-450b-d17c-43799ea54f71"
      },
      "source": [
        "cd /content/gdrive/My Drive/42028-DL-CNN-2020/assignment-3/"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/42028-DL-CNN-2020/assignment-3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-e7QJ8snsGWl",
        "colab_type": "text"
      },
      "source": [
        "# Pre-process cropped images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfRpap22sGN1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "91cb9787-9b5d-457e-bce3-7a5b1ae3aa4e"
      },
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "import os\n",
        "\n",
        "# Apply canny edge detection \n",
        "def auto_canny(image, sigma=0.33):\n",
        "    v = np.median(image)\n",
        "    lower = int(max(0, (1.0 - sigma) * v))\n",
        "    upper = int(min(255, (1.0 + sigma) * v))\n",
        "    edged_image = cv.Canny(image, lower, upper)\n",
        " \n",
        "    return edged_image\n",
        "\n",
        "# Crops characters out of numerplate \n",
        "def crop_ctrs(img):\n",
        "    ret, mask = cv2.threshold(grayimage, 254, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    cv2.imshow('mask', mask)\n",
        "    cv2.waitKey(0)\n",
        "\n",
        "    image, contours, hierarchy = cv2.findContours(mask, cv2.RETR_LIST, \n",
        "    cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    for contour in contours:\n",
        "\n",
        "        if cv2.contourArea(contour) < 200:\n",
        "            continue\n",
        "\n",
        "        rect = cv2.minAreaRect(contour)\n",
        "        box = cv2.boxPoints(rect)\n",
        "\n",
        "        ext_left = tuple(contour[contour[:, :, 0].argmin()][0])\n",
        "        ext_right = tuple(contour[contour[:, :, 0].argmax()][0])\n",
        "        ext_top = tuple(contour[contour[:, :, 1].argmin()][0])\n",
        "        ext_bot = tuple(contour[contour[:, :, 1].argmax()][0])\n",
        "\n",
        "        roi_corners = np.array([box], dtype=np.int32)\n",
        "\n",
        "        cv2.polylines(bounding_box_image, roi_corners, 1, (255, 0, 0), 3)\n",
        "        cv2.imshow('image', bounding_box_image)\n",
        "        cv2.waitKey(0)\n",
        "\n",
        "        cropped_image = grayimage[ext_top[1]:ext_bot[1], ext_left[0]:ext_right[0]]\n",
        "        cv2.imwrite('crop.jpg', cropped_image)\n",
        "\n",
        "path = \"/content/gdrive/My Drive/42028-DL-CNN-2020/assignment-3/cropped_dataset/plates/train/\"\n",
        "bounding_boxes = []\n",
        "counter = 0\n",
        "while counter < 1001:\n",
        "    for image_path in os.listdir(path):\n",
        "        full_image_path = os.path.join(path, image_path)\n",
        "        img = cv.imread(full_image_path)\n",
        "\n",
        "        # Loop through each image, apply pre-processing & localize onto each character\n",
        "        gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
        "        thresh_inv = cv.adaptiveThreshold(gray,255,cv.ADAPTIVE_THRESH_MEAN_C,cv.THRESH_BINARY_INV,39,1)\n",
        "        edges = auto_canny(thresh_inv)\n",
        "        ctrs, _ = cv.findContours(edges.copy(), cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
        "        sorted_ctrs = sorted(ctrs, key=lambda ctr: cv.boundingRect(ctr)[0])\n",
        "\n",
        "        img_area = img.shape[0]*img.shape[1]\n",
        "\n",
        "        # Get bounding box co-ordinates for image cropping\n",
        "        for i, ctr in enumerate(sorted_ctrs):\n",
        "            x, y, w, h = cv.boundingRect(ctr)\n",
        "            roi_area = w*h\n",
        "            roi_ratio = roi_area/img_area\n",
        "\n",
        "            if((roi_ratio >= 0.04) and (roi_ratio < 0.18)):\n",
        "                    if ((h>1.2*w) and (3*w>=h)):\n",
        "                        cv.rectangle(img,(x,y),( x + w, y + h ),(90,0,255), 1)\n",
        "                        bounding_boxes.append((x,y,w,h))\n",
        "        counter += 1\n",
        "\n",
        "# Crop bounding boxes and save into new dir\n",
        "count = 0\n",
        "for box in bounding_boxes:\n",
        "    x,y,w,h = box\n",
        "    ROI = img[y:y+h, x:x+w]\n",
        "    cv.imwrite('/content/gdrive/My Drive/42028-DL-CNN-2020/assignment-3/cropped_dataset/characters/train/char_{}.png'.format(str(count)), ROI)\n",
        "    count += 1\n",
        "    cv2_imshow(ROI)\n",
        "    cv.waitKey()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-59-8d9c98a883d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mfull_image_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_image_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;31m# Loop through each image, apply pre-processing & localize onto each character\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXCLLBTs8JAF",
        "colab_type": "text"
      },
      "source": [
        "# Generate images using PIL with similar font\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXy17D_Z8IxX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "alphanumerics = [\"A\", \"a\",\"B\",\"b\",\"C\",\"c\",\"D\",\"d\",\"E\",\"e\",\"F\",\"f\",\"G\",\"g\",\"H\",\"h\",\"I\",\"J\",\"K\",\"k\",\"L\",\"l\",\"M\",\"m\",\"N\",\"n\",\"O\",\"o\",\"P\",\"p\",\"Q\",\"q\",\"R\",\n",
        "                \"r\",\"S\",\"s\",\"T\",\"t\",\"U\",\"u\",\"V\",\"v\",\"W\",\"w\",\"X\",\"x\",\"Y\",\"y\",\"Z\",\"z\",\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\"]\n",
        "\n",
        "def generate_images():\n",
        "    for char in alphanumerics:\n",
        "        img = Image.new('RGB', (60, 60), color = (255, 255, 255))\n",
        "        fnt = ImageFont.truetype('/content/gdrive/My Drive/42028-DL-CNN-2020/assignment-3/cropped_dataset/characters/fonts/DIN 1451 Std Engschrift.otf', 34)\n",
        "        d = ImageDraw.Draw(img)\n",
        "        d.text((20,20), char, font=fnt, fill=(0, 0, 0))\n",
        "\n",
        "        img.save('/content/gdrive/My Drive/42028-DL-CNN-2020/assignment-3/cropped_dataset/characters/scrap/char_{}.png'.format(char))\n",
        "\n",
        "generate_images()\n",
        "\n",
        "\n",
        "# initialise image generator on synthesized alphanumeric images, apply image augmentation - skew, rotation, scale"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VorxjpD09o8i",
        "colab_type": "text"
      },
      "source": [
        "# Deep CNN for character image classification\n",
        "\n",
        "The localized numberplate characters are cropped and used as input data for an image classification CNN model which will have 35 target classes (all alphanumericals)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wXXmhtoiCe1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model():\n",
        "    model = models.Sequential()\n",
        "    model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(height, width, channel)))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(layers.MaxPooling2D((2, 2)))\n",
        "    model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "    model.add(layers.Dense(35, activation='softmax'))\n",
        "    model.summary()\n",
        "    model.compile(optimizer='adam',\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_images, train_labels, epochs=8)\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)\n",
        "model.save(\"model_char_recognition.h5\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}